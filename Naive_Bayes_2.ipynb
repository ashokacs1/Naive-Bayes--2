{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
        "\n",
        "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "\n",
        "### Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
      ],
      "metadata": {
        "id": "068KtwPMk4_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
        "\n",
        "This is a classic application of **conditional probability** and **Bayes' theorem**. The question asks for the probability that an employee is a smoker given that they use the health insurance plan.\n",
        "\n",
        "### Given:\n",
        "- \\( P(H) = 0.70 \\): Probability that an employee uses the health insurance plan.\n",
        "- \\( P(S | H) = 0.40 \\): Probability that an employee is a smoker given that they use the health insurance plan.\n",
        "\n",
        "We need to calculate \\( P(S | H) \\), which is already provided as 0.40. In this case, no further calculations are needed because the probability of being a smoker given the employee uses the plan is already directly provided. Therefore:\n",
        "\n",
        "\\[\n",
        "P(\\text{Smoker | Health Insurance}) = 0.40\n",
        "\\]\n",
        "\n",
        "So, the probability that an employee is a smoker given that they use the health insurance plan is **40%**.\n",
        "\n",
        "---\n",
        "\n",
        "## Q2: What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "\n",
        "**Bernoulli Naive Bayes** and **Multinomial Naive Bayes** are two types of Naive Bayes classifiers, and they differ mainly in how they handle feature data.\n",
        "\n",
        "### 1. **Bernoulli Naive Bayes:**\n",
        "- **Assumes binary features**: In Bernoulli Naive Bayes, each feature is treated as a binary variable (0 or 1). This model is best suited for tasks where the presence or absence of a feature matters.\n",
        "- **Example usage**: It is commonly used in text classification where the features represent whether a particular word is present or absent in a document (e.g., spam filtering, sentiment analysis).\n",
        "- **Feature handling**: It only considers whether a feature is present (1) or absent (0). Therefore, the model will penalize instances where a feature is missing from a class.\n",
        "\n",
        "### 2. **Multinomial Naive Bayes:**\n",
        "- **Assumes discrete feature counts**: Multinomial Naive Bayes is used when the features represent discrete counts. This model is suitable when the features represent frequencies or occurrences (non-negative integer values).\n",
        "- **Example usage**: It is typically used for text classification where the features are word counts or frequencies (e.g., the number of times a word appears in a document).\n",
        "- **Feature handling**: Unlike Bernoulli Naive Bayes, it captures the count or frequency of features and adjusts the probability calculations accordingly. It assumes that the more a word appears, the more significant it is for classification.\n",
        "\n",
        "### Summary of Key Differences:\n",
        "- **Bernoulli NB** is for binary data (presence/absence), while **Multinomial NB** is for count data (frequency of features).\n",
        "- Bernoulli NB penalizes for missing features, while Multinomial NB focuses on frequency distributions.\n",
        "\n",
        "---\n",
        "\n",
        "## Q3: How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "In **Bernoulli Naive Bayes**, missing values are implicitly handled by treating the missing feature as absent. Since the model is designed to work with binary feature data, it expects each feature to either be present (1) or absent (0). Therefore, if a value for a feature is missing, Bernoulli Naive Bayes interprets it as the absence of the feature (i.e., assigns a value of 0).\n",
        "\n",
        "### Example:\n",
        "For instance, if you're classifying text data and a specific word is missing from a document, Bernoulli Naive Bayes treats this as the absence of the word, just as it would if the word simply did not occur in the document. In this case, the model would calculate the probabilities using the 0 value for the missing feature.\n",
        "\n",
        "While Bernoulli Naive Bayes does not have a built-in method to deal with missing data in the traditional sense (like imputation), it treats the absence of data as meaningfulâ€”i.e., the feature does not contribute to the classification.\n",
        "\n",
        "### Limitations:\n",
        "However, this method can be problematic if the missing data is not random or if it conveys some specific meaning. In such cases, it might be necessary to preprocess the data by imputing missing values before applying the Bernoulli Naive Bayes model.\n",
        "\n",
        "---\n",
        "\n",
        "## Q4: Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "\n",
        "Yes, **Gaussian Naive Bayes** can be used for **multi-class classification**. The Gaussian Naive Bayes model assumes that the features are continuous and follow a normal (Gaussian) distribution. It is commonly used for tasks where the input features are continuous variables.\n",
        "\n",
        "### Working of Gaussian Naive Bayes in Multi-class Classification:\n",
        "\n",
        "- For **multi-class classification**, Gaussian Naive Bayes works by applying the same Naive Bayes principles, but instead of calculating probabilities for two classes (binary classification), it calculates the probabilities for each of the multiple classes.\n",
        "- The model estimates the likelihood of each class by calculating the probability of the data given each class, using the Gaussian (normal) distribution for the features.\n",
        "- During the prediction, the model assigns the class label that has the highest posterior probability.\n",
        "\n",
        "### Example:\n",
        "For instance, if we have a dataset with three possible classes \\( C_1, C_2, C_3 \\), Gaussian Naive Bayes would calculate:\n",
        "\n",
        "\\[\n",
        "P(C_1|X) = \\frac{P(X|C_1) \\cdot P(C_1)}{P(X)}\n",
        "\\]\n",
        "\\[\n",
        "P(C_2|X) = \\frac{P(X|C_2) \\cdot P(C_2)}{P(X)}\n",
        "\\]\n",
        "\\[\n",
        "P(C_3|X) = \\frac{P(X|C_3) \\cdot P(C_3)}{P(X)}\n",
        "\\]\n",
        "\n",
        "Where \\( X \\) represents the feature values. The class with the highest posterior probability is selected as the predicted class.\n",
        "\n",
        "### Application in Multi-class:\n",
        "Gaussian Naive Bayes can be applied to problems like:\n",
        "- **Iris dataset classification**, where the task is to classify a flower into one of three species based on continuous measurements (e.g., sepal length, petal width, etc.).\n",
        "- **Wine quality classification**, where the model predicts the quality of wine (multi-class labels) based on continuous features like alcohol content, acidity, etc.\n",
        "\n",
        "In conclusion, Gaussian Naive Bayes is a powerful tool for multi-class classification tasks, especially when the feature data is continuous and follows a Gaussian distribution."
      ],
      "metadata": {
        "id": "euu4MOGcnWQL"
      }
    }
  ]
}